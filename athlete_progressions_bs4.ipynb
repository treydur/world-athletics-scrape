{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(options=options, service=ChromeService(ChromeDriverManager().install()))\n",
    "driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_athlete_list = []\n",
    "athlete_list = []\n",
    "\n",
    "for x in range(1,12):\n",
    "    currPage = f'https://worldathletics.org/world-rankings/1500m/men?regionType=world&page={x}&rankDate=2022-08-09&limitByCountry=0'\n",
    "\n",
    "    # so can get max number of pages\n",
    "    try:\n",
    "        driver.get(currPage)\n",
    "        driver.implicitly_wait(10)\n",
    "    except:\n",
    "        break\n",
    "\n",
    "    page = driver.page_source\n",
    "\n",
    "    sp = BeautifulSoup(page, 'lxml') #lxml is faster than python's built in parser\n",
    "\n",
    "    trTags = sp.select('table.records-table > tbody > tr')\n",
    "    for trTag in trTags:\n",
    "        # Get these details now before the DOM changes\n",
    "        name = trTag.select_one('td[data-th=\"Competitor\"]').text.strip().replace('\\n','')\n",
    "        dob = trTag.select_one('td[data-th=\"DOB\"]').text.strip().replace('\\n','')\n",
    "        nat = trTag.select_one('td[data-th=\"Nat\"]').text.strip().replace('\\n','')\n",
    "        score = int(trTag.select_one('td[data-th=\"score\"]').text.strip().replace('\\n',''))\n",
    "        events = trTag.select_one('td[data-th=\"EventList\"]').text.strip().replace('\\n','')\n",
    "\n",
    "        # some of the links wouldn't work because the data-athlete-url had a 0 in it\n",
    "        link = 'https://worldathletics.org' + (trTag.attrs['data-athlete-url'].replace('-0','-'))\n",
    "        \n",
    "        base_athlete_item = {\n",
    "            'name': name,\n",
    "            'dob': dob,\n",
    "            'nat': nat,\n",
    "            'score': score,\n",
    "            'events': events,\n",
    "            'link': link,\n",
    "        }\n",
    "        base_athlete_list.append(base_athlete_item)\n",
    "    \n",
    "    # Go to each athletes page and scrape their progression charts\n",
    "    for y in base_athlete_list:\n",
    "        try:\n",
    "            driver.get(y['link'])\n",
    "            driver.implicitly_wait(10)\n",
    "            driver.execute_script(\n",
    "                \"arguments[0].click();\",\n",
    "                driver.find_elements(\n",
    "                        By.CSS_SELECTOR,\n",
    "                        'div.profileStatistics_tab__1Blal'\n",
    "                )[4]\n",
    "            )\n",
    "            time.sleep(1)\n",
    "            page = driver.page_source\n",
    "        except:\n",
    "            print(\"no selection\")\n",
    "            continue\n",
    "\n",
    "        sp = BeautifulSoup(page, 'lxml')\n",
    "        wholeTable = sp.select_one('div.profileStatistics_tabContent__vINmY.profileStatistics_active__1QQ9F')\n",
    "        tableDivs = wholeTable.select('div.profileStatistics_statsTable__xU9PN')\n",
    "        for div in tableDivs:\n",
    "            k = div.select('div.profileStatistics_tableName__2qDVZ')\n",
    "            environment = k[0].text\n",
    "            race = k[1].text\n",
    "            trs = div.select('tbody.profileStatistics_tableBody__1w5O9 > tr')\n",
    "            for tr in trs:\n",
    "                performance = tr.select_one('td:nth-child(2)').text\n",
    "                place = tr.select_one('td:nth-child(3)').text\n",
    "                date = tr.select_one('td:nth-child(4)').text\n",
    "                athlete_item = {\n",
    "                    'name': y['name'],\n",
    "                    'dob': y['dob'],\n",
    "                    'nat': y['nat'],\n",
    "                    'score': y['score'],\n",
    "                    'events': y['events'],\n",
    "                    'environment': environment,\n",
    "                    'race': race,\n",
    "                    'date': date,\n",
    "                    'performance': performance,\n",
    "                    'place': place\n",
    "                }\n",
    "                athlete_list.append(athlete_item)\n",
    "                print(athlete_item)\n",
    "    # Chunk the data per page into individual csv files\n",
    "    print(f'page {x} done')\n",
    "    pd.DataFrame(athlete_list).to_csv(f\"pg_{x}.csv\")\n",
    "    athlete_list = []\n",
    "    base_athlete_list = []\n",
    "\n",
    "    # Took 65m 9.9s to run this code against 10 pages of rankings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('webscraping')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a41dcd05efbd48bfcdfd7713256c5cd3709418c0a4fdd82ffc128e928590378"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
